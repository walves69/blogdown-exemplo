getwd()
library(twitteR)
library(httr)
# Carregando a biblioteca com funÃ§Ãµes de limpeza
source('utils.R')
# Chaves de autenticaÃ§Ã£o no Twitter
key <- "xQY0OH1WDriR0RGW7YQsQxDdR"
secret <- "W8NLDHzG6MN1mr0wx30MfaK9JIegWysnZdQPE3vJchGr3sUwFi"
token <- "44461711-Sr4fgvAE5WkzXvEeFNIsU04Mc21nZPkCOG0TECsXT"
tokensecret <- "PbalkEWLKlTJLzCJ39bztebe2qhyfGugNx1DpiQXTjYTV"
bearer <- "AAAAAAAAAAAAAAAAAAAAAEkGEQEAAAAAYJl57e10Jqvr2RZ2gPeLjXP1CzI%3D96mSmsUtqIyGx9TYDAJkWzeaeZnJZBCqne1a1bkIibfjaS4cG8"
# AutenticaÃ§Ã£o. Responda 1 quando perguntado sobre utilizar direct connection.
setup_twitter_oauth(key, secret, token, tokensecret)
# Verificando a timeline do usuÃ¡rio
userTimeline("dsacademybr")
close_trends=closestTrendLocations(-5.321160,-39.336788)
head(close_trends)
qtd_tweets <- 50
lingua <- "pt"
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua, geocode='-5.321160,-39.336788,450km')
# Capturando os tweets
tema <- "vacina"
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua, geocode='-5.321160,-39.336788,450km')
# Visualizando as primeiras linhas do objeto tweetdata
head(tweetdata)
close_trends=closestTrendLocations("-5.321160,-39.336788,450km")
close_trends=closestTrendLocations(-5.321160,-39.336788,450)
head(close_trends)
# Verificando a timeline do usuário
userTimeline("dsacademybr")
# Capturando os tweets
tema <- "BigData"
qtd_tweets <- 100
lingua <- "pt"
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua)
# Visualizando as primeiras linhas do objeto tweetdata
head(tweetdata)
# Instalando o pacote para Text Mining.
install.packages("tm")
install.packages("SnowballC")
library(SnowballC)
library(tm)
library(tm)
library(SnowballC)
library(tm)
options(warn=-1)
# Tratamento (limpeza, organização e transformação) dos dados coletados
tweetlist <- sapply(tweetdata, function(x) x$getText())
tweetlist <- iconv(tweetlist, to = "utf-8", sub="")
tweetlist <- limpaTweets(tweetlist)
tweetcorpus <- Corpus(VectorSource(tweetlist))
tweetcorpus <- tm_map(tweetcorpus, removePunctuation)
tweetcorpus <- tm_map(tweetcorpus, content_transformer(tolower))
tweetcorpus <- tm_map(tweetcorpus, function(x)removeWords(x, stopwords()))
# Convertendo o objeto Corpus para texto plano
# tweetcorpus <- tm_map(tweetcorpus, PlainTextDocument)
termo_por_documento = as.matrix(TermDocumentMatrix(tweetcorpus), control = list(stopwords = c(stopwords("portuguese"))))
# Instalando o pacote wordcloud
install.packages("RColorBrewer")
install.packages("wordcloud")
library(RColorBrewer)
library(wordcloud)
# Gerando uma nuvem palavras
pal2 <- brewer.pal(8,"Dark2")
wordcloud(tweetcorpus,
min.freq = 2,
scale = c(5,1),
random.color = F,
max.word = 60,
random.order = F,
colors = pal2)
# Convertendo o objeto texto para o formato de matriz
tweettdm <- TermDocumentMatrix(tweetcorpus)
tweettdm
# Encontrando as palavras que aparecem com mais frequência
findFreqTerms(tweettdm, lowfreq = 11)
# Buscando associações
findAssocs(tweettdm, 'datascience', 0.60)
# Removendo termos esparsos (não utilizados frequentemente)
tweet2tdm <- removeSparseTerms(tweettdm, sparse = 0.9)
# Criando escala nos dados
tweet2tdmscale <- scale(tweet2tdm)
# Distance Matrix
tweetdist <- dist(tweet2tdmscale, method = "euclidean")
# Preparando o dendograma
tweetfit <- hclust(tweetdist)
# Criando o dendograma (verificando como as palvras se agrupam)
plot(tweetfit)
# Verificando os grupos
cutree(tweetfit, k = 4)
# Visualizando os grupos de palavras no dendograma
rect.hclust(tweetfit, k = 3, border = "red")
# Criando uma função para avaliar o sentimento
install.packages("stringr")
install.packages("plyr")
library(stringr)
library(plyr)
sentimento.score = function(sentences, pos.words, neg.words, .progress = 'none')
{
# Criando um array de scores com lapply
scores = laply(sentences,
function(sentence, pos.words, neg.words)
{
sentence = gsub("[[:punct:]]", "", sentence)
sentence = gsub("[[:cntrl:]]", "", sentence)
sentence =gsub('\\d+', '', sentence)
tryTolower = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
sentence = sapply(sentence, tryTolower)
word.list = str_split(sentence, "\\s+")
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress = .progress )
scores.df = data.frame(text = sentences, score = scores)
return(scores.df)
}
# Mapeando as palavras positivas e negativas
pos = readLines("palavras_positivas.txt")
neg = readLines("palavras_negativas.txt")
# Criando massa de dados para teste
teste = c("Big Data is the future", "awesome experience",
"analytics could not be bad", "learn to use big data")
# Testando a função em nossa massa de dados dummy
testesentimento = sentimento.score(teste, pos, neg)
class(testesentimento)
# Verificando o score
# 0 - expressão não possui palavra em nossas listas de palavras positivas e negativas ou
# encontrou uma palavra negativa e uma positiva na mesma sentença
# 1 - expressão possui palavra com conotação positiva
# -1 - expressão possui palavra com conotação negativa
testesentimento$score
# Tweets por país
catweets = searchTwitter("ca", n = 500, lang = "en")
usatweets = searchTwitter("usa", n = 500, lang = "en")
# Obtendo texto
catxt = sapply(catweets, function(x) x$getText())
usatxt = sapply(usatweets, function(x) x$getText())
# Vetor de tweets dos países
paisTweet = c(length(catxt), length(usatxt))
# Juntando os textos
paises = c(catxt, usatxt)
# Aplicando função para calcular o score de sentimento
scores = sentimento.score(paises, pos, neg, .progress = 'text')
# Calculando o score por país
scores$paises = factor(rep(c("ca", "usa"), paisTweet))
scores$muito.pos = as.numeric(scores$score >= 1)
scores$muito.neg = as.numeric(scores$score <= -1)
# Calculando o total
numpos = sum(scores$muito.pos)
numneg = sum(scores$muito.neg)
# Score global
global_score = round( 100 * numpos / (numpos + numneg) )
head(scores)
boxplot(score ~ paises, data = scores)
# Gerando um histograma com o lattice
install.packages("lattice")
library("lattice")
histogram(data = scores, ~score|paises, main = "Análise de Sentimentos", xlab = "", sub = "Score")
install.packages("Rstem_0.4-1.tar.gz", sep = "", repos = NULL, type = "source")
install.packages("sentiment_0.2.tar.gz",sep = "", repos = NULL, type = "source")
install.packages("ggplot2")
library(Rstem)
install.packages("Rstem_0.4-1.tar.gz", sep = "", repos = NULL, type = "source")
install.packages("sentiment_0.2.tar.gz",sep = "", repos = NULL, type = "source")
install.packages("ggplot2")
library(sentiment)
library(ggplot2)
# Coletando os tweets
tweetpt = searchTwitter("bigdata", n = 1500, lang = "pt")
# Obtendo o texto
tweetpt = sapply(tweetpt, function(x) x$getText())
# Removendo caracteres especiais
tweetpt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweetpt)
# Removendo @
tweetpt = gsub("@\\w+", "", tweetpt)
# Removendo pontuação
tweetpt = gsub("[[:punct:]]", "", tweetpt)
# Removendo digitos
tweetpt = gsub("[[:digit:]]", "", tweetpt)
# Removendo links html
tweetpt = gsub("http\\w+", "", tweetpt)
# Removendo espacos desnecessários
tweetpt = gsub("[ \t]{2,}", "", tweetpt)
tweetpt = gsub("^\\s+|\\s+$", "", tweetpt)
# Criando função para tolower
try.error = function(x)
{
# Criando missing value
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
# Lower case
tweetpt = sapply(tweetpt, try.error)
# Removendo os NAs
tweetpt = tweetpt[!is.na(tweetpt)]
names(tweetpt) = NULL
# Classificando emocao
class_emo = classify_emotion(tweetpt, algorithm = "bayes", prior = 1.0)
emotion = class_emo[,7]
# Substituindo NA's por "Neutro"
emotion[is.na(emotion)] = "Neutro"
# Classificando polaridade
class_pol = classify_polarity(tweetpt, algorithm = "bayes")
polarity = class_pol[,4]
# Gerando um dataframe com o resultado
sent_df = data.frame(text = tweetpt, emotion = emotion,
polarity = polarity, stringsAsFactors = FALSE)
# Ordenando o dataframe
sent_df = within(sent_df,
emotion <- factor(emotion, levels = names(sort(table(emotion),
decreasing=TRUE))))
# Emoções encontradas
ggplot(sent_df, aes(x = emotion)) +
geom_bar(aes(y = ..count.., fill = emotion)) +
scale_fill_brewer(palette = "Dark2") +
labs(x = "Categorias", y = "Numero de Tweets")
# Polaridade
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x = "Categorias de Sentimento", y = "Numero de Tweets")
# Quais as trends disponíveis para o Ceará ?
close_trends=closestTrendLocations(-5.321160,-39.336788,450)
head(close_trends)
# Apenas Fortaleza. Vamos usar o woeid 455830 para buscar as tendências dessa região
trends=getTrends(455830)
head(trends)
head(strip_retweets(tweets,strip_manual=TRUE,strip_mt=TRUE))
head(strip_retweets(tweetdata,strip_manual=TRUE,strip_mt=TRUE))
head(strip_retweets(tweetdata,strip_manual=TRUE,strip_mt=TRUE))
# Buscando dados de usuário
crantastic<-getUser('crantastic')
crantastic$getDescription()
crantastic$getFollowersCount()
crantastic$getFriends(n=5)
crantastic$getFavorites(n=5)
df<-twListToDF(tweetdata)
head(df)
View(df)
View(df)
cran_tweets<-userTimeline('cranatic')
cran_tweets[1:5]
# Trends
avail_trends=availableTrendLocations()
head(avail_trend)
# Trends
avail_trends=availableTrendLocations()
head(avail_trends)
r_tweets<-searchTwitter("#rstats",n=50)
View(r_tweets)
View(r_tweets)
r_tweets[[1]][[".->statusSource"]]
sources<-sapply(r_tweets,function(x)x$getStatusSource())
sources
sources<-gsub("</a>","",sources)
sources
sources<-strsplit(sources,">")
sources
View(sources)
sources[[1]]
# O padrão é que quando enviado da web não tenha nenhum link
# Para os demais, sim, haverá um link
# A lógica aqui é, se tiver link não foi da web
sources<-sapply(sources,function(x)ifelse(length(x)>1,x[2],x[1]))
sources
source_table=table(sources)
source_table
r_tweets<-searchTwitter("#rstats",n=100)
sources<-sapply(r_tweets,function(x)x$getStatusSource())
sources
sources<-gsub("</a>","",sources)
sources
# Quebrou o que sobrou da linha no sinal de maior que fecha o link.
# Assim temos no primeiro elemento do vetor o link e no segundo o conteúdo do link
# Conteúdo da tag <a>
sources<-strsplit(sources,">")
sources
# Se para cada elemento do vetor eu tiver mais de um elemento pegar o segundo (conteúdo do link)
# caso contrário pegar o primeiro visto que quando foi feito o split só devia ter um
sources<-sapply(sources,function(x)ifelse(length(x)>1,x[2],x[1]))
sources
source_table=table(sources)
source_table
pie(source_table[source_table>5])
pie(source_table[source_table>10])
pie(source_table[source_table>=6])
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua, geocode='-5.321160,-39.336788,450km')
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua, geocode='-5.321160,-39.336788,450km')
qtd_tweets <- 200
lingua <- "pt"
tweetdata = searchTwitter(tema, n = qtd_tweets, lang = lingua, geocode='-5.321160,-39.336788,450km')
sources<-sapply(tweetdata,function(x)x$getStatusSource())
sources
# sources<-sapply(r_tweets,function(x)x$getStatusSource())
sources<-sapply(tweetdata,function(x)x$getStatusSource())
sources
sources<-gsub("</a>","",sources)
sources
key <- Sys.getenv("TWITTER_KEY")
key
usethis::create_project("C:\r\blogdown-exemplo2")
usethis::create_project("C:\\r\\blogdown-exemplo2")
setwd("C:/r/walves.blog.br")
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
